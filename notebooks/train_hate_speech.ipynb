{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "train-hate-speech",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install git+https://github.com/cosmoquester/transformers-tf-finetune.git"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ih731fUio05R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import csv\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AdamWeightDecay, AutoTokenizer\n",
        "\n",
        "from transformers_tf_finetune.models import TFBartForSequenceMultiClassification\n",
        "from transformers_tf_finetune.utils import LRScheduler, get_device_strategy, path_join, set_random_seed"
      ],
      "outputs": [],
      "metadata": {
        "id": "vrvYJ8reqBFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "xjkDo06wuRAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#: transformers pretrained path\n",
        "pretrained_model = \"cosmoquester/bart-ko-small\"\n",
        "#: pretrained tokenizer fast pretrained path\n",
        "pretrained_tokenizer = \"cosmoquester/bart-ko-small\"\n",
        "#: load from pytorch weight\n",
        "from_pytorch = False\n",
        "#: use huggingface credential for private model\n",
        "use_auth_token = \"\"\n",
        "\n",
        "train_dataset_path = \"https://raw.githubusercontent.com/kocohub/korean-hate-speech/master/labeled/train.tsv\"\n",
        "dev_dataset_path = \"https://raw.githubusercontent.com/kocohub/korean-hate-speech/master/labeled/dev.tsv\"\n",
        "#: output directory to save log and model checkpoints, should be GCS path with TPU\n",
        "output_path = None\n",
        "\n",
        "#: training params\n",
        "epochs = 5\n",
        "learning_rate = 5e-5\n",
        "min_learning_rate = 1e-5\n",
        "warmup_rate = 0.06\n",
        "warmup_steps = None\n",
        "batch_size = 128\n",
        "dev_batch_size = 512\n",
        "num_valid_dataset = 500\n",
        "tensorboard_update_freq = 1\n",
        "\n",
        "#: device to use (TPU or GPU or CPU)\n",
        "device = \"TPU\"\n",
        "#: Use mixed precision FP16\n",
        "mixed_precision = False\n",
        "#: Set random seed\n",
        "seed = None"
      ],
      "outputs": [],
      "metadata": {
        "id": "1S1L2-t7qRxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if output_path is not None and output_path.startswith(\"gs://\"):\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "outputs": [],
      "metadata": {
        "id": "wqP3q8KZvTXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def load_dataset(dataset_path: str, tokenizer: AutoTokenizer, shuffle: bool = False) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Load Hate Speech dataset from local file or web\n",
        "\n",
        "    :param dataset_path: local file path or file uri\n",
        "    :param tokenizer: PreTrainedTokenizer for tokenizing\n",
        "    :param shuffle: whether shuffling lines or not\n",
        "    :returns: Hate Speech dataset, number of dataset\n",
        "    \"\"\"\n",
        "    if dataset_path.startswith(\"https://\"):\n",
        "        with urllib.request.urlopen(dataset_path) as response:\n",
        "            data = response.read().decode(\"utf-8\")\n",
        "    else:\n",
        "        with open(dataset_path) as f:\n",
        "            data = f.read()\n",
        "    lines = data.splitlines()[1:]\n",
        "    if shuffle:\n",
        "        random.shuffle(lines)\n",
        "\n",
        "    bos = tokenizer.bos_token\n",
        "    eos = tokenizer.eos_token\n",
        "\n",
        "    bias_label2id = {\"none\": 0, \"gender\": 1, \"others\": 2}\n",
        "    hate_label2id = {\"none\": 0, \"hate\": 1, \"offensive\": 2}\n",
        "\n",
        "    sentences = []\n",
        "    bias_labels = []\n",
        "    hate_labels = []\n",
        "    for comment, _, bias_label, hate_label in csv.reader(lines, delimiter=\"\\t\"):\n",
        "        sentences.append(bos + comment + eos)\n",
        "        bias_labels.append(bias_label2id[bias_label])\n",
        "        hate_labels.append(hate_label2id[hate_label])\n",
        "\n",
        "    inputs = dict(\n",
        "        tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            return_tensors=\"tf\",\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, {\"bias\": bias_labels, \"hate\": hate_labels}))\n",
        "    return dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "NZ6nno7nrgQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if seed:\n",
        "    set_random_seed(seed)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BTej5sm8r5Jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "strategy = get_device_strategy(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ka-cxaFGsMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Precision"
      ],
      "metadata": {
        "id": "p4xdNRu2tiZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    if mixed_precision:\n",
        "        mixed_type = \"mixed_bfloat16\" if device == \"TPU\" else \"mixed_float16\"\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(mixed_type)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)"
      ],
      "outputs": [],
      "metadata": {
        "id": "A0UhTl2BtIIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "VWVZH3-VuWHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "        tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer, use_auth_token=use_auth_token)\n",
        "\n",
        "        dataset = load_dataset(train_dataset_path, tokenizer, True)\n",
        "        train_dataset = dataset.skip(num_valid_dataset).batch(batch_size)\n",
        "        valid_dataset = dataset.take(num_valid_dataset).batch(dev_batch_size)\n",
        "        dev_dataset = load_dataset(dev_dataset_path, tokenizer).batch(dev_batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "a73NxJD5tk8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "dYL-lnWOuZgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    model = TFBartForSequenceMultiClassification.from_pretrained(\n",
        "        pretrained_model,\n",
        "        list_num_labels={\"bias\": 3, \"hate\": 3},\n",
        "        use_auth_token=use_auth_token,\n",
        "        from_pt=from_pytorch,\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "LLPZGWhdttFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Compile"
      ],
      "metadata": {
        "id": "7s_B01rauc7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    model.compile(\n",
        "        optimizer=AdamWeightDecay(\n",
        "            LRScheduler(\n",
        "                len(train_dataset) * epochs,\n",
        "                learning_rate,\n",
        "                min_learning_rate,\n",
        "                warmup_rate,\n",
        "                warmup_steps,\n",
        "            ),\n",
        "            weight_decay_rate=0.01,\n",
        "            exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n",
        "        ),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "cdIgKdMVtw0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "Kp5JeBv-ueUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                path_join(output_path, \"best_model.ckpt\"),\n",
        "                save_weights_only=True,\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_loss\",\n",
        "                mode=\"min\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(\n",
        "                path_join(output_path, \"logs\"), update_freq=tensorboard_update_freq\n",
        "            ),\n",
        "        ] if output_path is not None else None,\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "a34uOyuQtz71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluate"
      ],
      "metadata": {
        "id": "DGFUZNarugBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    total_loss, bias_loss, hate_loss, bias_accuracy, hate_accuracy = model.evaluate(dev_dataset)"
      ],
      "outputs": [],
      "metadata": {
        "id": "64m9DkPzt7XJ"
      }
    }
  ]
}