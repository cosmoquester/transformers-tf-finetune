{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih731fUio05R"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/cosmoquester/transformers-tf-finetune.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrvYJ8reqBFV"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "from transformers_tf_finetune.losses import SparseCategoricalCrossentropy\n",
        "from transformers_tf_finetune.metrics import SparseCategoricalAccuracy\n",
        "from transformers_tf_finetune.models import GenerationSearchWrapper\n",
        "from transformers_tf_finetune.utils import LRScheduler, get_device_strategy, path_join, set_random_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkDo06wuRAW"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S1L2-t7qRxD"
      },
      "outputs": [],
      "source": [
        "#: transformers pretrained path\n",
        "pretrained_model = \"cosmoquester/bart-ko-small\"\n",
        "#: pretrained tokenizer fast pretrained path\n",
        "pretrained_tokenizer = \"cosmoquester/bart-ko-small\"\n",
        "#: load from pytorch weight\n",
        "from_pytorch = False\n",
        "#: use huggingface credential for private model\n",
        "use_auth_token = \"\"\n",
        "\n",
        "dataset_path = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
        "#: output directory to save log and model checkpoints, should be GCS path with TPU\n",
        "output_path = None\n",
        "\n",
        "max_sequence_length = 128\n",
        "#: \"beam size, use greedy search if this is zero\"\n",
        "beam_size = 0\n",
        "\n",
        "#: training params\n",
        "epochs = 2\n",
        "learning_rate = 1e-4\n",
        "min_learning_rate = 1e-5\n",
        "warmup_rate = 0.06\n",
        "warmup_steps = None\n",
        "batch_size = 16\n",
        "dev_batch_size = 256\n",
        "num_dev_dataset = 128 # should be multipes of 8 with TPU\n",
        "tensorboard_update_freq = 1\n",
        "\n",
        "#: device to use (TPU or GPU or CPU)\n",
        "device = \"TPU\"\n",
        "#: Use mixed precision FP16\n",
        "mixed_precision = False\n",
        "#: Set random seed\n",
        "seed = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqP3q8KZvTXg"
      },
      "outputs": [],
      "source": [
        "if output_path is not None and output_path.startswith(\"gs://\"):\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ6nno7nrgQH"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path: str, tokenizer: AutoTokenizer, shuffle: bool = False) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Load Chatbot Conversation dataset from local file or web\n",
        "\n",
        "    :param dataset_path: local file path or file uri\n",
        "    :param tokenizer: PreTrainedTokenizer for tokenizing\n",
        "    :param shuffle: whether shuffling lines or not\n",
        "    :returns: conversation dataset\n",
        "    \"\"\"\n",
        "    if dataset_path.startswith(\"https://\"):\n",
        "        with urllib.request.urlopen(dataset_path) as response:\n",
        "            data = response.read().decode(\"utf-8\")\n",
        "    else:\n",
        "        with open(dataset_path) as f:\n",
        "            data = f.read()\n",
        "    lines = data.splitlines()[1:]\n",
        "    if shuffle:\n",
        "        random.shuffle(lines)\n",
        "\n",
        "    bos = tokenizer.bos_token or tokenizer.cls_token or \"\"\n",
        "    eos = tokenizer.eos_token or tokenizer.sep_token\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for question, answer, _ in csv.reader(lines):\n",
        "        questions.append(bos + question + eos)\n",
        "        answers.append(bos + answer + eos)\n",
        "\n",
        "    max_length = max(len(text) for text in questions + answers)\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"tf\",\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "    target_tokens = tokenizer(\n",
        "        answers,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"tf\",\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=False,\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ({**inputs, \"decoder_input_ids\": target_tokens[:, :-1]}, target_tokens[:, 1:])\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTej5sm8r5Jj"
      },
      "outputs": [],
      "source": [
        "if seed:\n",
        "    set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka-cxaFGsMqr"
      },
      "outputs": [],
      "source": [
        "strategy = get_device_strategy(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4xdNRu2tiZq"
      },
      "source": [
        "# Mixed Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0UhTl2BtIIQ"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    if mixed_precision:\n",
        "        mixed_type = \"mixed_bfloat16\" if device == \"TPU\" else \"mixed_float16\"\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(mixed_type)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWVZH3-VuWHh"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a73NxJD5tk8s"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer, use_auth_token=use_auth_token)\n",
        "\n",
        "    dataset = load_dataset(dataset_path, tokenizer, True)\n",
        "    train_dataset = dataset.skip(num_dev_dataset).batch(batch_size)\n",
        "    dev_dataset = dataset.take(num_dev_dataset).batch(dev_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYL-lnWOuZgm"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLPZGWhdttFm"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = TFAutoModelForSeq2SeqLM.from_pretrained(\n",
        "        pretrained_model, use_auth_token=use_auth_token, from_pt=from_pytorch, use_cache=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s_B01rauc7S"
      },
      "source": [
        "# Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdIgKdMVtw0q"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model.compile(\n",
        "        optimizer=tf.optimizers.Adam(\n",
        "            LRScheduler(\n",
        "                len(train_dataset) * epochs,\n",
        "                learning_rate,\n",
        "                min_learning_rate,\n",
        "                warmup_rate,\n",
        "                warmup_steps,\n",
        "            )\n",
        "        ),\n",
        "        loss=SparseCategoricalCrossentropy(from_logits=True, ignore_index=tokenizer.pad_token_id),\n",
        "        metrics=SparseCategoricalAccuracy(ignore_index=tokenizer.pad_token_id, name=\"accuracy\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp5JeBv-ueUN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a34uOyuQtz71"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=dev_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                path_join(output_path, \"best_model.ckpt\"),\n",
        "                save_weights_only=True,\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_accuracy\",\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(\n",
        "                path_join(output_path, \"logs\"), update_freq=tensorboard_update_freq\n",
        "            ),\n",
        "        ] if output_path is not None else None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGFUZNarugBN"
      },
      "source": [
        "# Model Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64m9DkPzt7XJ"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    loss, accuracy = model.evaluate(dev_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytifNJOp7nVn"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7D-oTpp7oGL"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    input_tokens = []\n",
        "    predict_tokens = []\n",
        "    ppls = []\n",
        "    searcher = GenerationSearchWrapper(\n",
        "        model,\n",
        "        max_sequence_length,\n",
        "        tokenizer.convert_tokens_to_ids(tokenizer.bos_token),\n",
        "        tokenizer.convert_tokens_to_ids(tokenizer.eos_token),\n",
        "        tokenizer.convert_tokens_to_ids(tokenizer.pad_token),\n",
        "        beam_size=beam_size,\n",
        "    )\n",
        "    for batch, _ in strategy.experimental_distribute_dataset(dev_dataset):\n",
        "        if beam_size > 0:\n",
        "            output, ppl = strategy.run(searcher.beam_search, args=(batch[\"input_ids\"], batch[\"attention_mask\"]))\n",
        "            output = strategy.gather(output, axis=0)[:, 0, :]\n",
        "            ppl = strategy.gather(ppl, axis=0)[:, 0]\n",
        "        else:\n",
        "            output, ppl = strategy.run(searcher.greedy_search, args=(batch[\"input_ids\"], batch[\"attention_mask\"]))\n",
        "            output = strategy.gather(output, axis=0)\n",
        "            ppl = strategy.gather(ppl, axis=0)\n",
        "        input_tokens.extend(strategy.gather(batch[\"input_ids\"], axis=0).numpy())\n",
        "        predict_tokens.extend(output.numpy())\n",
        "        ppls.extend(ppl.numpy())\n",
        "\n",
        "    input_sentences = tokenizer.batch_decode(input_tokens, skip_special_tokens=True)\n",
        "    predict_sentences = tokenizer.batch_decode(predict_tokens, skip_special_tokens=True)\n",
        "    for question, answer, ppl in zip(input_sentences, predict_sentences, ppls):\n",
        "        print(f\"Q: {question} A: {answer} PPL:{ppl:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train-chatbot",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
